---
---
@article{firefly,
  title={FireFly: A Synthetic Dataset for Ember Detection in Wildfire},
  author={Hu, Yue and Ye, Xinan and Liu, Yifei and Kundu, Souvik and Datta, Gourav and Mutnuri, Srikar and Asavisanu, Namo and Ayanian, Nora and Psounis, Konstantinos and Beerel, Peter},
  journal={IEEE/CVF International Conference on Computer Vision (CVPR)},
  pages={3765--3769},
  year={2023},
  arxiv={2308.03164},
  pdf={https://openaccess.thecvf.com/content/ICCV2023W/AIHADR/papers/Hu_FireFly_A_Synthetic_Dataset_for_Ember_Detection_in_Wildfire_ICCVW_2023_paper.pdf},
  selected={true},
  preview={firefly_ember.gif},
  abstract={This paper presents "FireFly", a synthetic dataset for ember detection created using Unreal Engine 4 (UE4), designed to overcome the current lack of ember-specific training resources. To create the dataset, we present a tool that allows the automated generation of the synthetic labeled dataset with adjustable parameters, enabling data diversity from various environmental conditions, making the dataset both diverse and customizable based on user requirements. We generated a total of 19,273 frames that have been used to evaluate FireFly on four popular object detection models. Further to minimize human intervention, we leveraged a trained model to create a semi-automatic labeling process for real-life ember frames. Moreover, we demonstrated an up to 8.57% improvement in mean Average Precision (mAP) in real-world wildfire scenarios compared to models trained exclusively on a small real dataset.}
}

@article{firestereo,
  title={FIReStereo: Forest InfraRed Stereo Dataset for UAS Depth Perception in Visually Degraded Environments},
  author={Dhrafani*, Devansh and Liu*, Yifei and Jong, Andrew and Shin, Ukcheol and He, Yao and Harp, Tyler and Hu, Yaoyu and Oh, Jean and Scherer, Sebastian},
  journal={IEEE Robotics and Automation Letters},
  year={2025},
  arxiv={2409.07715},
  website={https://firestereo.github.io/},
  selected={true},
  preview={firestereo.gif},
  abstract={Robust depth perception in degraded visual environments is crucial for autonomous aerial systems. Thermal imaging cameras, which capture infrared radiation, are robust to visual degradation. However, due to lack of a large-scale dataset, the use of thermal cameras for unmanned aerial system (UAS) depth perception has remained largely unexplored. This paper presents a stereo thermal depth perception dataset for autonomous aerial perception applications. The dataset consists of stereo thermal images, LiDAR, IMU and ground truth depth maps captured in urban and forest settings under diverse conditions like day, night, rain, and smoke. We benchmark representative stereo depth estimation algorithms, offering insights into their performance in degraded conditions. Models trained on our dataset generalize well to unseen smoky conditions, highlighting the robustness of stereo thermal imaging for depth perception. We aim for this work to enhance robotic perception in disaster scenarios, allowing for exploration and operations in previously unreachable areas.}
}

@article{thermaldiffusion,
  title={ThermalDiffusion: Visual-to-Thermal Image-to-Image Translation for Autonomous Navigation},
  author={Bansal*, Shruti and Wang, Wenshan, and Liu*, Yifei and Maheshwari, Parv},
  journal={IEEE International Conference on Robotics and Automation (ICRA)},
  year={2025},
  arxiv={2506.20969},
  pdf={https://arxiv.org/pdf/2506.20969},
  selected={true},
  abstract={Autonomous systems rely on sensors to estimate
  the environment around them. However, cameras, LiDARs, and
  RADARs have their own limitations. In nighttime or degraded
  environments such as fog, mist, or dust, thermal cameras can
  provide valuable information regarding the presence of objects
  of interest due to their heat signature. They make it easy
  to identify humans and vehicles that are usually at higher
  temperatures compared to their surroundings. In this paper,
  we focus on the adaptation of thermal cameras for robotics
  and automation, where the biggest hurdle is the lack of data.
  Several multi-modal datasets are available for driving robotics
  research in tasks such as scene segmentation, object detection,
  and depth estimation, which are the cornerstone of autonomous
  systems. However, they are found to be lacking in thermal
  imagery. Our paper proposes a solution to augment these
  datasets with synthetic thermal data to enable widespread and
  rapid adaptation of thermal cameras. We explore the use of
  conditional diffusion models to convert existing RGB images
  to thermal images using self-attention to learn the thermal
  properties of real-world objects.}
}

@article{redraw,
  title={Adapting World Models with Latent-State Dynamics Residuals},
  author={Lanier, JB and Kim, Kyungmin and Karamzade, Armin and Liu, Yifei and Sinha, Ankita and He, Kathleen and Corsi, Davide and Fox, Roy},
  journal={Under review},
  year={2025},
  arxiv={2504.02252},
  selected={true},
  website={https://redraw.jblanier.net/},
  abstract={Simulation-to-reality reinforcement learning (RL) faces the challenge of reconciling discrepancies between simulated and real-world dynamics, which can degrade agent performance. When real data is scarce, a promising approach involves learning corrections to simulator forward dynamics represented as a residual error function, however this operation is impractical with high-dimensional states such as images. To overcome this, we propose ReDRAW, a latent-state autoregressive world model pretrained in simulation and calibrated to a target environment through residual corrections of latent-state dynamics rather than of explicit observed states. Using this adapted world model, ReDRAW enables RL agents to be optimized with imagined rollouts under corrected dynamics and then deployed in the real world. In multiple vision-based DeepMind Control Suite domains and a physical robot visual lane-following task, ReDRAW effectively models changes to dynamics and avoids overfitting in low data regimes where traditional transfer methods fail.}
}