---
---
@article{firefly,
  title={FireFly: A Synthetic Dataset for Ember Detection in Wildfire},
  author={Hu, Yue and Ye, Xinan and Liu, Yifei and Kundu, Souvik and Datta, Gourav and Mutnuri, Srikar and Asavisanu, Namo and Ayanian, Nora and Psounis, Konstantinos and Beerel, Peter},
  journal={IEEE/CVF International Conference on Computer Vision (CVPR)},
  pages={3765--3769},
  year={2023},
  arxiv={2308.03164},
  pdf={https://openaccess.thecvf.com/content/ICCV2023W/AIHADR/papers/Hu_FireFly_A_Synthetic_Dataset_for_Ember_Detection_in_Wildfire_ICCVW_2023_paper.pdf},
  selected={true},
  preview={firefly_ember.gif},
  abstract={This paper presents "FireFly", a synthetic dataset for ember detection created using Unreal Engine 4 (UE4), designed to overcome the current lack of ember-specific training resources. To create the dataset, we present a tool that allows the automated generation of the synthetic labeled dataset with adjustable parameters, enabling data diversity from various environmental conditions, making the dataset both diverse and customizable based on user requirements. We generated a total of 19,273 frames that have been used to evaluate FireFly on four popular object detection models. Further to minimize human intervention, we leveraged a trained model to create a semi-automatic labeling process for real-life ember frames. Moreover, we demonstrated an up to 8.57% improvement in mean Average Precision (mAP) in real-world wildfire scenarios compared to models trained exclusively on a small real dataset.}
}

@article{firestereo,
  title={FIReStereo: Forest InfraRed Stereo Dataset for UAS Depth Perception in Visually Degraded Environments},
  author={Dhrafani*, Devansh and Liu*, Yifei and Jong, Andrew and Shin, Ukcheol and He, Yao and Harp, Tyler and Hu, Yaoyu and Oh, Jean and Scherer, Sebastian},
  journal={ArXiv},
  year={2024},
  arxiv={2409.07715},
  website={https://firestereo.github.io/},
  selected={true},
  preview={firestereo.gif},
  abstract={Robust depth perception in degraded visual environments is crucial for autonomous aerial systems. Thermal imaging cameras, which capture infrared radiation, are robust to visual degradation. However, due to lack of a large-scale dataset, the use of thermal cameras for unmanned aerial system (UAS) depth perception has remained largely unexplored. This paper presents a stereo thermal depth perception dataset for autonomous aerial perception applications. The dataset consists of stereo thermal images, LiDAR, IMU and ground truth depth maps captured in urban and forest settings under diverse conditions like day, night, rain, and smoke. We benchmark representative stereo depth estimation algorithms, offering insights into their performance in degraded conditions. Models trained on our dataset generalize well to unseen smoky conditions, highlighting the robustness of stereo thermal imaging for depth perception. We aim for this work to enhance robotic perception in disaster scenarios, allowing for exploration and operations in previously unreachable areas.}
}
